from typing import Callable, Dict, Any, List, Optional
import asyncio
from loguru import logger

async def retry_llm_call(
    llm_call: Callable,  # The LLM function to call (e.g., call_litellm_structured)
    llm_config: Dict,  # Configuration for the LLM call
    validation_strategies: List[Callable],  # List of validation functions
    max_retries: int = 3,  # Maximum number of retries
) -> Optional[Dict]:
    """
    A generic function to handle retries, validation, and iterative improvement for LLM calls.

    Args:
        llm_call (Callable): The LLM function to call.
        llm_config (Dict): Configuration for the LLM call.
        validation_strategies (List[Callable]): List of functions to validate the LLM response.
        max_retries (int): Maximum number of retries.

    Returns:
        Optional[Dict]: The validated LLM response, or None if max retries are exceeded.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = await llm_call(llm_config)
            validation_errors = []

            # Apply all validation strategies
            for validate in validation_strategies:
                validation_result = validate(response)
                if validation_result is not True:
                    validation_errors.append(validation_result)

            # If all validations pass, return the response
            if not validation_errors:
                return response

            # If any validation fails, log the errors and retry
            logger.warning(f"Validation failed: {', '.join(validation_errors)}")
            llm_config["messages"].append({
                "role": "assistant",
                "content": f"Validation errors: {', '.join(validation_errors)}. Please correct the response."
            })
        except Exception as e:
            logger.error(f"Attempt {retries + 1} failed: {e}")
        retries += 1

    # Max retries exceeded: Add the failure reason to the messages object
    failure_message = f"Max retries ({max_retries}) exceeded. The LLM failed to generate a valid response."
    llm_config["messages"].append({
        "role": "assistant",
        "content": failure_message
    })
    logger.error(failure_message)
    raise Exception(failure_message)

async def main():
    return

if __name__ == "__main__":
    asyncio.run(main())